(window.webpackJsonp=window.webpackJsonp||[]).push([[60],{442:function(t,v,_){"use strict";_.r(v);var a=_(54),s=Object(a.a)({},(function(){var t=this,v=t.$createElement,_=t._self._c||v;return _("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[_("h1",{attrs:{id:"数据采集与预处理"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#数据采集与预处理"}},[t._v("#")]),t._v(" 数据采集与预处理")]),t._v(" "),_("h2",{attrs:{id:"数据采集"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#数据采集"}},[t._v("#")]),t._v(" 数据采集")]),t._v(" "),_("p",[t._v("数据采集又称为数据获取，是使用技术手段对离线数据和实时数据采集的过程，是数据分析中的重要环节。离线数据主要包含业务数据和网络数据，实时数据包含日志数据和传感器设备数据。")]),t._v(" "),_("p",[t._v("数据采集要注重全面性，也就是要根据某一特定需求，采集的数据量要足够多，数据面要涉及的足够广，这样才能准确进行分析。例如做商品推荐时，需要对用户的年龄、性别、地区、购买行为、浏览时长、购买收藏等行为进行全面的数据收集和分析，才能最终得出推荐给用户的商品是什么这个结论。数据采集也要从多种不同的维度来描述数据，很多不关联维度的信息相互关联，通过数据挖掘可以得出有价值的数据。所以，信息采集目的需要明确，要带着问题去收集各个维度的信息，注重数据采集时间的间隔，使得信息采集更高效、更有针对性。")]),t._v(" "),_("p",[t._v("数据采集的数据源包含："),_("strong",[t._v("业务数据、网络数据、系统日志数据和传感器数据")]),t._v("。业务数据是指业务系统在运行过程中产生的数据直接记录在数据库中，这些存储在数据库中的业务数据是采集的数据源之一。例如用户账号、购买的商品、商品的评论等数据信息。网络数据泛指在互联网中特定的数据形式进行呈现的数据，如，评论信息、弹幕、图片、音频、视频、文件等。系统日志数据一般是由业务系统在运行过程中产生的，用于记录数据源相关的操作，这些操作的记录以文本的形式进行存储。日志用来记录用户的活动数据或者系统的异常，以文本的形式保存。传感器数据是由传感器设备检测出来的信息，如摄像头、电子温度计、心电图仪器或者其他的智能终端设备采集的信号、图片或录像等，这些数据信息都可以作为数据采集的对象进行收集。")]),t._v(" "),_("p",[t._v("针对不同的数据源都不同的技术手段来完成采集的步骤。可以用sqoop在Hadoop与传统数据库之间进行数据传输，可以用于传统数据库与分布式数据库之间的数据迁移。用Flume采集海量的日志，也使用Kafka分布式发布消息和进行消息订阅，Kafka可以对日志数据和实时数据进行采集。如果是类似网页内容这类非结构化的数据，可以利用爬虫技术将非结构化的数据从网页中抽取出来，以结构化的形式存储在本地。")]),t._v(" "),_("h2",{attrs:{id:"数据预处理"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#数据预处理"}},[t._v("#")]),t._v(" 数据预处理")]),t._v(" "),_("p",[t._v("数据预处理的主要流程包含：数据清洗、数据集成、数据变换和数据规约。")]),t._v(" "),_("p",[_("strong",[t._v("数据清洗是指识别不完整、不准确、不相关的脏数据，然后替换、修改、删除该脏数据的过程")]),t._v("。数据清洗的内容主要包含：缺失值处理、异常值处理、数据类型转换以及重复值处理。")]),t._v(" "),_("p",[t._v("缺失值是指调查有遗漏、编码和录入的误差，数据可能存在一些缺失值，需要给与适当的处理。常用的处理方法是利用样本均值、中位数或者众数来替代缺失值，或者通过变量之间的关联分析或逻辑推论进行估计。也可以使用整例删除、变量删除、成对删除等方式剔除缺失值样本。")]),t._v(" "),_("p",[t._v("异常值处理需要探讨每个变量的合理取值范围和相互关系，检查数据是否合乎要求，如果超出正常范围，逻辑上不合理或者相互矛盾，就会把这类数据归类为异常值。异常值通常使用删除、插补、转换和分箱值等方式进行处理。")]),t._v(" "),_("p",[t._v("数据类型不一致也会影响后续数据分析处理环节，因此，需要明确每个字段的数据类型，并统一进行处理。比如A表当中的年月日是字符串类型，B表当中的字符串是日期类型，这种数据不一致的情况会影响数据分析环节，所以要统一数据类型，要将其进行转换，才能后续统一利用这些数据分析挖掘。")]),t._v(" "),_("p",[t._v("重复值是由于各种原因，数据中可能存在重复记录和重复字段，这些重复值会影响数据分析和数据挖掘的准确性，所以如果存在重复值就需要对其进行处理，如果存在重复值，就需要将其删除。为了避免重复值的产生，可以通过系统的业务来规避，比如某种业务场景下，只允许用户提交一次数据写入操作。")]),t._v(" "),_("p",[t._v("数据清洗时，我们要先处理缺失值、异常值和数据类型转换，最后进行重复值处理。对缺失值和异常值处理时，一般根据业务需求对其进行填充，或者进行统计值填充、前后值填充、零值填充。数据清洗之前，要明确表的结构和发现需要处理的值，方便数据清洗的更彻底。数据量大、异常数据量较小时，可以直接删除异常数据，数据量小时，每个数据都可能影响分析结果，需要认真对异常值进行处理。最后要通过质量保证手段，确保数据表的每列都进行清洗。")]),t._v(" "),_("p",[t._v("数据集成是将互相有关联的多个异构数据源中的数据集成到一起，逻辑上或物理上形成统一的、可靠的和高质量的数据集合，为企业提供全面的数据共享，并让用户能够以统一的方式访问这些数据。数据集成的目的是打破数据源之间的信息不能共享，造成系统中存在大量的冗余数据、垃圾数据，无法保证数据的一致性。可以用联邦数据库、数据仓库或者全局数据模型来访问异构数据源来完成数据集成。")]),t._v(" "),_("p",[t._v("数据集成要解决实体识别、冗余数据分析、数据冲突和检测这三个问题。实体识别指匹配多个数据源中的等价实体，比如同义不同名的数据，或者同名不同义的数据。冗余问题分为三类，分别是：属性重复、属性相关数据重复和元组重复。数据冲突和检测表示我们对同一实体、来自不同数据源的属性定义不同，例如表示方法、度量单位、编码等内容。")]),t._v(" "),_("p",[t._v("数据变换的目的是将数据转换成容易进行数据挖掘的形式，常见的数据变换策略有：平滑处理、规范化处理、属性构造处理、聚集处理和数据泛化处理等几种方式。数据规约指在尽可能保持数据原貌的前提下，最大限度的精简数据量，通过数据规约，获取一个保持数据原貌的精简数据集合，可以提高数据分析和挖掘的效率。")])])}),[],!1,null,null,null);v.default=s.exports}}]);